HYBRID SEARCH FLOW DIAGRAM
===========================

┌─────────────────────────────────────────────────────────────────────┐
│                        USER QUERY                                   │
│                  "Tell me about aachem"                             │
└──────────────────────┬──────────────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────────────┐
│                  GENERATE RAG QUERY (LLM)                           │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │ Input: user query + conversation history (if follow-up)       │ │
│  │ First message? Skip LLM, return query as-is                   │ │
│  │ Follow-up? Call Gemini with optimization prompt               │ │
│  │ Output: JSON with optimized query + tags                      │ │
│  │                                                                │ │
│  │ {                                                              │ │
│  │   "query": "aachem dataset variables chemistry",              │ │
│  │   "tags": ["aachem"]                                          │ │
│  │ }                                                              │ │
│  └────────────────────────────────────────────────────────────────┘ │
└──────────────────────┬──────────────────────────────────────────────┘
                       │
         ┌─────────────┴─────────────┐
         │                           │
         ▼                           ▼
    ┌─────────────┐        ┌──────────────────┐
    │ Tags Found? │        │ Optimized Query  │
    │ ["aachem"]  │        │ "aachem dataset  │
    │             │        │  variables..."   │
    └─────────────┘        └──────────────────┘
         │                           │
    YES │                           │
         │                           │
         ▼                           ▼
    ┌──────────────────────┐   ┌──────────────────────────┐
    │ KEYWORD MATCHING     │   │ SEMANTIC SEARCH (FAISS)  │
    │ search_by_tags()     │   │ search_variables()       │
    │                      │   │                          │
    │ For each tag:        │   │ Embed query + search     │
    │ - variable_name      │   │ Return top 10 by         │
    │   contains tag       │   │ cosine similarity        │
    │ - dataset            │   │                          │
    │   contains tag       │   │ Returns: ~10 results     │
    │                      │   │                          │
    │ Returns: ~20 chunks  │   │                          │
    │ (all aachem vars)    │   │                          │
    └──────────────────────┘   └──────────────────────────┘
         │                           │
         │ Keyword Matches          │ Semantic Matches
         │ [{                        │ [{
         │   variable_name: "AL3...", │   variable_name: "AL3...",
         │   dataset: "aachem",      │   dataset: "aachem",
         │   match_type: "dataset",  │   similarity_score: 0.87
         │   keyword_score: 1.0      │ }, ...]
         │ }, ...]                   │
         │                           │
         └─────────────┬─────────────┘
                       │
                       ▼
         ┌───────────────────────────┐
         │ COMBINE & DEDUPLICATE     │
         │                           │
         │ 1. Add keyword matches    │
         │ 2. Add semantic matches   │
         │    (no duplicates)        │
         │ 3. Keep order:            │
         │    keyword first,         │
         │    then semantic          │
         │                           │
         │ Result: ~149 chunks       │
         │ (140 keyword + 9 new      │
         │  semantic, no dupes)      │
         └───────────────┬───────────┘
                         │
                         ▼
         ┌───────────────────────────┐
         │ BUILD CONTEXT PROMPT      │
         │                           │
         │ "Here are 149 relevant    │
         │  variables retrieved from │
         │  CARDIA database...       │
         │  [formatted chunks]"      │
         └───────────────┬───────────┘
                         │
                         ▼
         ┌───────────────────────────┐
         │ LLM RESPONSE (Gemini)     │
         │                           │
         │ Input:                    │
         │ - System instructions     │
         │ - Chat history            │
         │ - Retrieved context       │
         │ - User query              │
         │                           │
         │ Output:                   │
         │ "The aachem dataset       │
         │  contains chemistry       │
         │  measurements including   │
         │  creatinine (AL3CREAT)... │
         │  Based on the 149         │
         │  variables retrieved..."  │
         └───────────────┬───────────┘
                         │
                         ▼
         ┌───────────────────────────┐
         │ SAVE DEBUG LOG            │
         │                           │
         │ data/debug/               │
         │ retrieved_chunks.json     │
         │                           │
         │ Shows:                    │
         │ - User query              │
         │ - Optimized query         │
         │ - Extracted tags          │
         │ - All chunks retrieved    │
         │ - Match metadata          │
         │ - Timestamps              │
         └───────────────┬───────────┘
                         │
                         ▼
         ┌───────────────────────────┐
         │ DISPLAY TO USER           │
         │                           │
         │ "The aachem dataset       │
         │  contains chemistry...    │
         │  [citations with          │
         │  variable names]"         │
         └───────────────────────────┘


TIMING BREAKDOWN
================

Step                          Time        % of Total
─────────────────────────────────────────────────
LLM Query Generation          2-3 sec     ~95%
  └─ Network latency to Google API
  └─ LLM processing
  └─ JSON parsing
                              
Keyword Matching              10-50 ms    <1%
  └─ String comparisons
  └─ In-memory operations
  
Semantic Search (FAISS)       ~100 ms     ~3%
  └─ Query embedding
  └─ Vector similarity
  
Other (dedup, format, etc)    ~50 ms      ~1%
─────────────────────────────────────────────────
TOTAL                         ~2.1-3.1s   100%
  

KEY INSIGHT: Keyword matching is FAST, LLM call is the bottleneck!


EXAMPLE RESULTS
===============

Query: "Tell me about aachem"
  Tags extracted: ["aachem"]
  Keyword matches: 142 (all aachem variables)
  Semantic matches: 10 (chemistry-related)
  Total sent to LLM: 152 (141 unique)
  
Query: "What blood pressure variables?"
  Tags extracted: [] (none)
  Keyword matches: 0
  Semantic matches: 10 (blood pressure + hypertension)
  Total sent to LLM: 10
  
Query: "Creatinine in aachem"
  Tags extracted: ["aachem", "AL3CREAT"]
  Keyword matches: 5 (aachem + creatinine)
  Semantic matches: 10 (kidney function)
  Total sent to LLM: 15 (after dedup)


COMPARISON: BEFORE vs AFTER
============================

BEFORE (Semantic-Only):
  Query: "Tell me about aachem"
  Results: 0 chunks (no semantic match on "aachem")
  LLM output: Hallucination (no context)
  ❌ FAILED

AFTER (Hybrid):
  Query: "Tell me about aachem"
  Results: 142 keyword matches + 10 semantic = 152 total
  LLM output: Accurate, well-cited
  ✅ SUCCESS

